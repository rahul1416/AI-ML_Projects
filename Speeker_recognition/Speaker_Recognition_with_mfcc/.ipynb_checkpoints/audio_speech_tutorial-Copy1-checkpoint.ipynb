{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM2VOkVBe2Qv"
   },
   "source": [
    "\n",
    "\n",
    "<center width=\"100%\">\n",
    "    <h1>Audio & Speech Processing -- Tutorial</h1>\n",
    "<h1>DS510 AI/ML Lab</h1>\n",
    "  \n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read the Audio file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "filename = 'talk.wav'\n",
    "# Extract data and sampling rate from file\n",
    "data, fs = sf.read(filename,dtype='float32')  #by default its float64\n",
    "\n",
    "print(type(data))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing Audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "print('playing sound using  playsound')\n",
    "sd.play(data, fs)\n",
    "status = sd.wait()\n",
    "\n",
    "\n",
    "\n",
    "#Using Simpleaudio\n",
    "# # !pip install simpleaudio\n",
    "# import simpleaudio as sa\n",
    "\n",
    "# audio_data = (data * (2**31 - 1)).astype('int32')\n",
    "\n",
    "# # Play the audio\n",
    "# play_obj = sa.play_buffer(audio_data, num_channels=1, bytes_per_sample=4, sample_rate=fs)\n",
    "\n",
    "# # Wait for the audio to finish playing\n",
    "# play_obj.wait_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(waveform, sample_rate):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    if num_channels == 1:\n",
    "        display(Audio(waveform[0], rate=sample_rate))\n",
    "    elif num_channels == 2:\n",
    "        display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
    "    else:\n",
    "        raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = get_speech_sample()\n",
    "display(Audio(data, rate=fs))\n",
    "# play_audio(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the audio in the form of waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the audio file\n",
    "filename = 'talk.wav'\n",
    "waveform, sample_rate = librosa.load(filename, sr=None)\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(waveform, sr=sample_rate)\n",
    "plt.title('Waveform of Audio Signal')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to write audio from numpy array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "rate = 44100\n",
    "data = np.random.uniform(-1, 1, rate) # 1 second worth of random samples between -1 and 1\n",
    "scaled = np.int16(data / np.max(np.abs(data)) * 32767)#scaling of data values\n",
    "write('test.wav', rate, scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fun Exercise: How to record in python?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egEs3RY-raId"
   },
   "source": [
    "Preparing data and utility functions (skip this section)\n",
    "--------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnJ8hffJFIYY"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wP4K-BVhbWq"
   },
   "outputs": [],
   "source": [
    "# uncomment this cell if running on colab\n",
    "# !pip install torchaudio librosa boto3\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.functional as F\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "#print(torch.__version__)\n",
    "#print(torchaudio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wGlE5WuG2bt"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  uncomment this cell if running on colab\n",
    "# !pip install --upgrade transformers datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1k_Kl3craId"
   },
   "outputs": [],
   "source": [
    "#In this tutorial, we will use a speech data from [VOiCES dataset](https://iqtlabs.github.io/voices/), \n",
    "#which is licensed under Creative Commos BY 4.0.\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# Preparation of data and helper functions.\n",
    "#-------------------------------------------------------------------------------\n",
    "import io\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import multiprocessing\n",
    "\n",
    "import scipy\n",
    "import librosa\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "#botocore library is used to configure the AWS client with an UNSIGNED configuration. \n",
    "#This is often used for public access to resources, \n",
    "#such as public datasets hosted on AWS S3 (Simple Storage Service), where authentication is not required. \n",
    "from botocore.config import Config\n",
    "import requests\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "[width, height] = matplotlib.rcParams['figure.figsize']\n",
    "if width < 10:\n",
    "    matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
    "\n",
    "_SAMPLE_DIR = \"_sample_data\"\n",
    "SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
    "SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
    "\n",
    "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
    "\n",
    "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
    "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
    "\n",
    "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
    "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
    "\n",
    "SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
    "SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
    "\n",
    "SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
    "SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
    "\n",
    "SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
    "SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
    "SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "S3_BUCKET = \"pytorch-tutorial-assets\"\n",
    "S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
    "os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
    "os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "#Defines a function _fetch_data() to download audio data files from the specified URLs and save them to the local paths.\n",
    "\n",
    "def _fetch_data():\n",
    "    uri = [\n",
    "    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
    "    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
    "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
    "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
    "    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
    "    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
    "    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
    "  ]\n",
    "    for url, path in uri:\n",
    "        with open(path, 'wb') as file_:\n",
    "            file_.write(requests.get(url).content)\n",
    "\n",
    "_fetch_data()\n",
    "\n",
    "#Defines a function _download_yesno() to download a specific dataset using torchaudio. \n",
    "#It checks if the dataset has already been downloaded before initiating the download process.\n",
    "\n",
    "def _download_yesno():\n",
    "    if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
    "        return\n",
    "    torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
    "\n",
    "YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
    "YESNO_DOWNLOAD_PROCESS.start()\n",
    "\n",
    "def _get_sample(path, resample=None):\n",
    "    effects = [\n",
    "    [\"remix\", \"1\"]\n",
    "  ]\n",
    "    if resample:\n",
    "        effects.extend([\n",
    "      [\"lowpass\", f\"{resample // 2}\"],\n",
    "      [\"rate\", f'{resample}'],\n",
    "    ])\n",
    "    return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].specgram(waveform[c], Fs=sample_rate)\n",
    "    if num_channels > 1:\n",
    "        axes[c].set_ylabel(f'Channel {c+1}')\n",
    "    if xlim:\n",
    "        axes[c].set_xlim(xlim)\n",
    "    figure.suptitle(title)\n",
    "    plt.show(block=False)\n",
    "\n",
    "\n",
    "\n",
    "def inspect_file(path):\n",
    "    print(\"-\" * 10)\n",
    "    print(\"Source:\", path)\n",
    "    print(\"-\" * 10)\n",
    "    print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
    "    print(f\" - {torchaudio.info(path)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgovlao5j2K8"
   },
   "source": [
    "\n",
    "How do we represent sound digitally?\n",
    "==================================\n",
    "\n",
    "**``To digitize a sound wave`` we must turn the signal into a series of numbers.**\n",
    "\n",
    "\n",
    "> In this way, we can serve it as input to our models.\n",
    ">\n",
    "> This is done by measuring the amplitude of the sound at ``fixed intervals of time``.\n",
    ">\n",
    "> Each such measurement is called a sample, and the ``sample rate`` is the number of samples per second.\n",
    "\n",
    "---\n",
    "\n",
    "This illustrated image image simply shows the discretization process (figure credit - [Discrete Fourier Transform](https://www.youtube.com/watch?v=mkGsMWi_j4Q))\n",
    "\n",
    "<center width=\"100%\">\n",
    "  <img src=\"https://raw.githubusercontent.com/koudounasalkis/Audio-Speech-Tutorial/main/images/waveform_digitalized.gif\" width=\"600px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xInQP5tN1Ejb"
   },
   "source": [
    "\n",
    "Spectrograms\n",
    "==================================\n",
    "\n",
    "Since a signal produces different sounds as it varies over time, its constituent frequencies also vary with time. In other words, its spectrum varies with time.\n",
    "\n",
    "**A spectrogram of a signal plots its spectrum over time and is like a ``photograph of the signal``**. It plots time on the x-axis and frequency on the y-axis.\n",
    "\n",
    "It uses different colors to indicate the Amplitude or strength of each frequency. Each vertical ‘slice’ of the spectrogram is the spectrum of the signal at a specific instant in time and it shows how the signal strength is distributed in every frequency found in the signal at that instant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Audio Deep Learning Models\n",
    "==================================\n",
    "\n",
    "Most deep learning audio applications use spectrograms to represent audio (or at least they did...)\n",
    "\n",
    "They usually follow a procedure like this:\n",
    "> ``Start with raw audio data`` in the form of a wave file.\n",
    ">\n",
    "> Convert the audio data into its ``corresponding spectrogram``.\n",
    ">\n",
    "> Optionally, use ``simple audio processing techniques`` to augment the spectrogram data. \n",
    ">\n",
    "> Use ``standard CNN architectures to process them and extract feature maps``, that are an encoded representation of the spectrogram image.\n",
    ">\n",
    "> ``Generate output predictions`` from this encoded representation, depending on the problem needed to be solved.\n",
    "\n",
    "---\n",
    "\n",
    "The image here shows a simple deep learning architecture for audio (figure credit - [iProbe: Spkr Recognition](https://iprobe.cse.msu.edu/project_detail.php?id=11&?title=Speaker_recognition_from_degraded_audio_samples))\n",
    "\n",
    "<center width=\"100%\">\n",
    "  <img src=\"https://raw.githubusercontent.com/koudounasalkis/Audio-Speech-Tutorial/main/images/deep_learning_architecture_example.png\" width=\"1200px\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions are wrappers for _get_sample, providing specific paths \n",
    "#for regular and speech audio samples. They allow optional resampling.\n",
    "\n",
    "def get_sample(*, resample=None):\n",
    "    \n",
    "    return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
    "\n",
    "def get_speech_sample(*, resample=None):\n",
    "      return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    axs.set_title(title or 'Spectrogram (db)')\n",
    "    axs.set_ylabel(ylabel)\n",
    "    axs.set_xlabel('frame')\n",
    "    im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
    "    if xmax:\n",
    "        axs.set_xlim((0, xmax))\n",
    "    fig.colorbar(im, ax=axs)\n",
    "    plt.show(block=False)\n",
    "    \n",
    "#plot_spectrogram is a function for visualizing a spectrogram using Matplotlib. \n",
    "#It sets up a subplot, adjusts labels, and displays the spectrogram with a colorbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(waveform, sample_rate=None, src=None):\n",
    "    if src:\n",
    "        print(\"-\" * 10)\n",
    "        print(\"Source:\", src)\n",
    "        print(\"-\" * 10)\n",
    "    if sample_rate:\n",
    "        print(\"Sample Rate:\", sample_rate)\n",
    "        print(\"Shape:\", tuple(waveform.shape))\n",
    "        print(\"Dtype:\", waveform.dtype)\n",
    "        print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
    "        print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
    "        print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
    "        print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
    "        print()\n",
    "        print(waveform)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "MPKa_m3N2Bm5",
    "outputId": "faf3c0c7-ec1c-4c0f-acef-5d20ff2ccb0c"
   },
   "outputs": [],
   "source": [
    "## Plot the spectrogram of an audio sample\n",
    "waveform, sample_rate = get_speech_sample()\n",
    "print(sample_rate)\n",
    "play_audio(waveform, sample_rate)\n",
    "print(\"\")\n",
    "\n",
    "n_fft = 1024\n",
    "win_length = None\n",
    "hop_length = 512\n",
    "\n",
    "# Define transformation import torchaudio.transforms as T\n",
    "spectrogram = T.Spectrogram(\n",
    "    n_fft=n_fft,#This parameter stands for \"number of Fourier Transform points.\" \n",
    "                       #It determines the number of data points used in each block for the Fast Fourier Transform (FFT)\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    pad_mode=\"reflect\", #    The padding method used if center is True. \n",
    "    #In this case, it's set to \"reflect,\" meaning the signal is padded by reflecting it at its boundaries.\n",
    "\n",
    "    power=2.0,\n",
    ")\n",
    "\n",
    "# Perform transformation\n",
    "spec = spectrogram(waveform)\n",
    "\n",
    "# print_stats(spec)\n",
    "plot_spectrogram(spec[0], title='Spectrogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm_rmTAkCqDZ"
   },
   "source": [
    "## Adding noise to speech sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "speech, _ = get_speech_sample(resample=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When simulating room impulse reverberation, you typically convolve an audio signal with the room impulse response. \n",
    "#This process mimics the effect of sound reflecting off surfaces in a room, creating reverberation. \n",
    "#The resulting audio signal will sound as if it was recorded in that simulated environment.\n",
    "\n",
    "\n",
    "def get_rir_sample(*, resample=None, processed=False):\n",
    "    # Get the raw impulse response (rir_raw) and sample rate from the specified path\n",
    "    rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
    "    #play_audio(rir,_)\n",
    " # If not processed, return the raw impulse response and sample rate\n",
    "    if not processed:\n",
    "        return rir_raw, sample_rate\n",
    "  # Extract a portion of the impulse response and normalize it\n",
    "    rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
    "    rir = rir / torch.norm(rir, p=2)\n",
    "    # Flip the impulse response along the time axis\n",
    "    rir = torch.flip(rir, [1])\n",
    "# Return the processed impulse response and sample rate\n",
    "    return rir, sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating Room Impulse Reverberation (RIR)\n",
    "# Using Room Impulse Response (RIR), we can make a clean speech sound like uttered in a conference room\n",
    "rir, _ = get_rir_sample(resample=sample_rate, processed=True)\n",
    "\n",
    "speech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))## Pad the speech signal to accommodate the impulse response\n",
    "speech = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n",
    "#Apply convolution to simulate the effect of RIR on the speech signal\n",
    "\n",
    "print(\"This is the audio uttered as it was recorded in a conference room, with the corresponding spectrogram\")\n",
    "plot_specgram(speech, sample_rate, title=\"RIR Applied\")\n",
    "print(\"\")\n",
    "play_audio(speech, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_sample(*, resample=None):\n",
    "      return _get_sample(SAMPLE_NOISE_PATH, resample=resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addding background noise to audio data\n",
    "noise, _ = get_noise_sample(resample=sample_rate)\n",
    "\n",
    "#This line ensures that the background noise (noise) has the same duration as the speech signal (speech). \n",
    "#It trims or extends the noise signal to match the length of the speech signal.\n",
    "noise = noise[:, :speech.shape[1]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set Signal-to-Noise Ratio (SNR) in decibels.\n",
    "#SNR is a measure of the ratio of the power of a signal to the power of background noise.\n",
    "snr_db = 8\n",
    "\n",
    "\n",
    "# Calculate scaling factor based on SNR\n",
    "scale = math.exp(snr_db / 10) * noise.norm(p=2) / speech.norm(p=2)\n",
    "\n",
    "#math.exp(snr_db / 10) converts the SNR from decibels to a linear scale.\n",
    "#noise.norm(p=2) calculates the L2 norm (Euclidean norm) of the noise signal, representing its energy.\n",
    "#speech.norm(p=2) calculates the L2 norm of the speech signal.\n",
    "#The formula scales the noise signal to achieve the desired SNR.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "speech = (scale * speech + noise) / 2\n",
    "# Mix the speech signal with background noise\n",
    "print(\"This is the audio with some background noise added, with the corresponding spectrogram\")\n",
    "plot_specgram(speech, sample_rate, title=\"BG noise added\")\n",
    "print(\"\")\n",
    "play_audio(speech, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Resampling of Audio file</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The purpose of resampling is often to change the sample rate of the audio data\n",
    "# To resample an audio waveform from one freqeuncy to another, \n",
    "# you can use transforms.Resample or functional.resample. \n",
    "# transforms.Resample precomputes and caches the kernel used for resampling, \n",
    "# while functional.resample computes it on the fly, so using transforms.Resample will result \n",
    "# in a speedup if resampling multiple waveforms using the same parameters\n",
    "sample_rate = 48000\n",
    "resample_rate = 32000\n",
    "resample_rate_1 = 16000\n",
    "\n",
    "waveform = get_sine_sweep(sample_rate)\n",
    "plot_sweep(waveform, sample_rate, title=\"Original Waveform\")\n",
    "print(\"\")\n",
    "play_audio(waveform, sample_rate)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\n",
    "#This creates a Resample transform with the specified original sample rate, target resample rate, and the dtype of the waveform.\n",
    "\n",
    "resampled_waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Can you perform the upsampling and downsampling and see the difference</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa    \n",
    "y, s = librosa.load('test.wav', sr=8000) # Downsample 44.1kHz to 8kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Applying Filters: Low Pass</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying filtering and changing sample rate\n",
    "speech, sample_rate = torchaudio.sox_effects.apply_effects_tensor(\n",
    "  waveform,\n",
    "  sample_rate,\n",
    "  effects=[\n",
    "      [\"lowpass\", \"100\"],\n",
    "      [\"rate\", \"16000\"],\n",
    "  ],\n",
    ")\n",
    "#This function is used to apply a chain of SoX (Sound eXchange) effects to an input waveform. \n",
    "#SoX is a command-line utility that can convert, process, and play various audio file formats.\n",
    "\n",
    "print(\"This is the audio after a low pass filter, with the corresponding spectrogram\")\n",
    "\n",
    "\n",
    "#Please plot the spectrogram here\n",
    "\n",
    "\n",
    "play_audio(speech, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exercise: Explore the High pass filter and band pass filter and write down your observations and about their working</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Feature Augmentation techniques with SpecAugment\n",
    "\n",
    "SpecAugment modifies the spectrogram by warping it in the time direction, masking blocks of consecutive frequency channels, and masking blocks of utterances in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(\n",
    "    n_fft = 400,\n",
    "    win_len = None,\n",
    "    hop_len = None,\n",
    "    power = 2.0,\n",
    "):\n",
    "    waveform, _ = get_speech_sample()\n",
    "    spectrogram = T.Spectrogram(\n",
    "      n_fft=n_fft,\n",
    "      win_length=win_len,\n",
    "      hop_length=hop_len,\n",
    "      center=True,\n",
    "      pad_mode=\"reflect\",\n",
    "      power=power,\n",
    "  )\n",
    "    return spectrogram(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some Feature Augmentation techniques with SpecAugment\n",
    "\n",
    "# Time Stretch\n",
    "#Time stretching allows you to modify the playback speed of an audio signal. Increasing the speed compresses the signal in time, \n",
    "#making it shorter, while decreasing the speed stretches it, making it longer.\n",
    "print(\"------- Time Stretch -------\")\n",
    "spec = get_spectrogram(power=None)\n",
    "strech = T.TimeStretch()#Initializes the TimeStretch transformation from the SpecAugment library.\n",
    "\n",
    "rate = 1.2\n",
    "spec_ = strech(spec, rate)#Applies the Time Stretch transformation to the original spectrogram with the specified rate.\n",
    "plot_spectrogram(spec_[0].abs(), title=f\"Stretched x{rate}\", aspect='equal', xmax=304)\n",
    "\n",
    "plot_spectrogram(spec[0].abs(), title=\"Original\", aspect='equal', xmax=304)\n",
    "\n",
    "rate = 0.9\n",
    "spec_ = strech(spec, rate)\n",
    "plot_spectrogram(spec_[0].abs(), title=f\"Stretched x{rate}\", aspect='equal', xmax=304)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Masking\n",
    "#Time masking involves masking consecutive time steps in the spectrogram or time-frequency representation of an audio signal. \n",
    "#The masked segments are essentially removed or replaced with zeros, creating gaps in the temporal information.\n",
    "print(\"------- Time Masking -------\")\n",
    "torch.random.manual_seed(4)\n",
    "\n",
    "spec = get_spectrogram()\n",
    "plot_spectrogram(spec[0], title=\"Original\")\n",
    "\n",
    "masking = T.TimeMasking(time_mask_param=80)#Initializes the TimeMasking transformation with a time mask parameter of 80.\n",
    "spec = masking(spec)#Applies Time Masking to the original spectrogram.\n",
    "\n",
    "plot_spectrogram(spec[0], title=\"Masked along time axis\")\n",
    "\n",
    "# Frequency Masking\n",
    "print(\"----- Frequency Masking -----\")\n",
    "torch.random.manual_seed(4)\n",
    "\n",
    "spec = get_spectrogram()\n",
    "plot_spectrogram(spec[0], title=\"Original\")\n",
    "\n",
    "masking = T.FrequencyMasking(freq_mask_param=80)\n",
    "spec = masking(spec)\n",
    "\n",
    "plot_spectrogram(spec[0], title=\"Masked along frequency axis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a21yQVUEJx3"
   },
   "source": [
    "\n",
    "What problems does audio deep learning solve?\n",
    "==================================\n",
    "\n",
    "**Audio data in day-to-day life can come in innumerable forms.**\n",
    "\n",
    "Given the prevalence of sounds in our lives and the range of sound types, it is not surprising that there are a huge number of scenarios that require us to process and analyze audio. Now that deep learning has come of age, it can be applied to solve a number of use cases.\n",
    "\n",
    "Here are some of the hot-topics nowadays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZVP4vaJE764"
   },
   "source": [
    "### Speaker Identification\n",
    "\n",
    "Here the task is to **identify the speaker from a given audio sample**. This is a very common task in the field of speech recognition and is used in many applications, such as voice biometrics.\n",
    "\n",
    "<center width=\"100%\">\n",
    "  <img src=\"https://raw.githubusercontent.com/koudounasalkis/Audio-Speech-Tutorial/main/images/audio_classification.png\" width=\"1200px\">\n",
    "</center>\n",
    "\n",
    "Figure credit: [tiensu](https://tiensu.github.io/blog/70_audio_deep_learning_part_4/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Part 1: Speaker Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks 1:\n",
    "1. Download the dataset from PTDB-TUG Dataset(subset of TIMIT)\n",
    "Read about the dataset here:\n",
    "https://www.spsc.tugraz.at/databases-and-tools/ptdb-tug-pitch-tracking-database-from-graz-university-of-technology.html\n",
    "\n",
    "Please find the below link for download: \n",
    "https://www.kaggle.com/datasets/lazyrac00n/speech-activity-detection-datasets\n",
    "\n",
    "\n",
    "2. Look at the structure of the dataset. Total utterances/speech samples present.\n",
    "\n",
    "3. Create Spectrograms for each utterance and store them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: \n",
    "\n",
    "1. Build a baseline network to perform the speaker identification using CNN\n",
    "2. Use Regularizers to avoid overfitting of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "from keras.layers import Lambda,Input\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def get_encoder(input_size):\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(32,(3,3),input_shape=(150,150,3),activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu'))\n",
    "    model.add(MaxPool2D(2,2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(GlobalMaxPool2D())\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_siamese_network(encoder,input_size):\n",
    "    input1=Input(input_size)\n",
    "    input2=Input(input_size)\n",
    "\n",
    "    encoder_l=encoder(input1)\n",
    "    encoder_r=encoder(input2)\n",
    "\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoder_l, encoder_r])\n",
    "\n",
    "    output=Dense(1,activation='sigmoid')(L1_distance)\n",
    "    siam_model=Model(inputs=[input1,input2],outputs=output)\n",
    "    return siam_model\n",
    "\n",
    "encoder=get_encoder((150,150,3))\n",
    "siamese_net=get_siamese_network(encoder,(150,150,3))\n",
    "siamese_net.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:\n",
    "Now add noise to your data and again perform the speaker identification. You can use audio augmentation so improve the accuracy.\n",
    "Train the model with noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ujqiwweE-fs"
   },
   "outputs": [],
   "source": [
    "#Wav2Vec2-Base for Speaker Identification: https://arxiv.org/abs/2006.11477\n",
    "#Transformer architecture, pretrained\n",
    "#Notebook for fine-tuning: https://colab.research.google.com/drive/1FjTsqbYKphl9kL-eILgUc-bl4zVThL8F?usp=sharing\n",
    "\n",
    "dataset = load_dataset(\"anton-l/superb_demo\", \"si\", split=\"test\")\n",
    "dataset\n",
    "classifier = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-sid\")\n",
    "\n",
    "label2name = {\n",
    "    \"id10003\": \"David Suchet\",\n",
    "    \"id10004\": \"Aaron Tveit\",\n",
    "    \"id10005\": \"Aaron Yoo\",\n",
    "    \"id10840\": \"India de Beaufort\",\n",
    "    \"id10005\": \"Abigail Breslin\",\n",
    "    \"id10008\": \"Kim Raver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = dataset[5][\"audio\"]\n",
    "display(Audio(audio[\"array\"], rate=audio[\"sampling_rate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = classifier(dataset[5][\"file\"], top_k=1)\n",
    "\n",
    "print(label)\n",
    "print(\"The speaker is: \", label2name[label[0][\"label\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Speech\n",
    "\n",
    "Conversely, with Speech Synthesis, or Text to Speech, one could go in the other direction and **take written text and generate speech from it**, using, for instance, an artificial voice for conversational agents.\n",
    "\n",
    "The most well-known examples that have achieved widespread use are virtual assistants like Alexa, Siri, Cortana, and Google Home, which are consumer-friendly products built around this capability.\n",
    "\n",
    "<center width=\"100%\">\n",
    "  <img src=\"https://raw.githubusercontent.com/koudounasalkis/Audio-Speech-Tutorial/main/images/text_to_speech.png\" height=\"10px\" width=\"600px\" >\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'tortoise-tts'\n",
      "/home/aldrax/Documents/SEM-6/AI-ML Lab/AI_ML_LAB/Assignment2\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tortoise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtortoise-tts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#!pip install unidecode\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# !pip install transformers==4.19.0\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# !pip install -r requirements.txt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# !python setup.py install\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtortoise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextToSpeech\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtortoise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_audio, load_voice, load_voices\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#https://docs.coqui.ai/en/latest/models/tortoise.html\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# This will download all the models used by Tortoise from the HuggingFace hub.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tortoise'"
     ]
    }
   ],
   "source": [
    "#@title Voice of the actor the model is going to simulate. {display-mode: \"form\"}\n",
    "#@markdown Don't look at this cell until you figured out the voice.\n",
    "\n",
    "voice = 'freeman'\n",
    "\n",
    "#!pip install -U scipy\n",
    "\n",
    "#!git clone https://github.com/jnordberg/tortoise-tts.git\n",
    "%cd tortoise-tts\n",
    "#!pip install unidecode\n",
    "# !pip install transformers==4.19.0\n",
    "# !pip install -r requirements.txt\n",
    "# !python setup.py install\n",
    "\n",
    "from tortoise.api import TextToSpeech\n",
    "from tortoise.utils.audio import load_audio, load_voice, load_voices\n",
    "\n",
    "#https://docs.coqui.ai/en/latest/models/tortoise.html\n",
    "\n",
    "# This will download all the models used by Tortoise from the HuggingFace hub.\n",
    "\n",
    "tts = TextToSpeech()\n",
    "\n",
    "# This is the text that will be spoken.\n",
    "text = \"While reading this short sentence, I am simulating the voice of a famous actor. \\\n",
    "    Are you able to guess who am I?\"\n",
    "\n",
    "# Pick a \"preset mode\" to determine quality. Options: {\"ultra_fast\", \"fast\" (default), \"standard\", \"high_quality\"}. See docs in api.py\n",
    "preset = \"high_quality\"\n",
    "\n",
    "voice_samples, conditioning_latents = load_voice(voice)\n",
    "gen = tts.tts_with_preset(text, voice_samples=voice_samples, conditioning_latents=conditioning_latents, \n",
    "                          preset=preset)\n",
    "torchaudio.save('../audio/tts_audio.wav', gen.squeeze(0).cpu(), 24000)\n",
    "%cd ../\n",
    "\n",
    "Audio('audio/tts_audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "50f798c039f92e39594af06ec0119751541d975fa6ec3b2f5528645cd2e370ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
